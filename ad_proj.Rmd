---
title: 'PROJECTE ANÀLISI DE DADES: Wine Quality'
author: "David Anglada Rotger i Andreu Huguet Segarra"
date: "17/5/2019"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    number_sections: yes
    toc: yes
fig_width: 5
fig_height: 3
---

# Introducció

Avui en dia, el rang de consumidors del vi s'ha amplitat moltíssims, convertint-se amb una beguda que es pot trobar a totes les cases i, fins i tot, comparable amb la cervesa. Això ha provocat que l'interés per aquest producte hagi augmentat els darrers anys, provocant també un creixement de la indústria de la vineria. Com a conseqüència, el nombre d'investigacions i estudis que tenen com a finalitat la millora de la qualitat del vi o la pujada de les seves vendes s'ha disparat.

Un dels temes que preocupa més a aquest sector és la **certificació de qualitat**, un aspecte que depèn profundament de la catació i valoració d'enòlegs experts. Així doncs, la finalitat d'aquest estudi és estudiar quines variables afecten més a la qualitat final del vi i de quina manera influeixen.

Donat que algunes d'aquestes variables es poden controlar durant el procés de producció, serà interessant veure les que influeixen possitivament amb l'objectiu de potenciar-les, o detectar les que influeixen negativament per poder intentar neutralitzar el seu efecte.

# Descripció de la Base de Dades

Per la realització d'aquest estudi es disposa d'una base de dades amb 1599 entrades diferents. Cada una es correspon amb la certificació de qualitat d'un vi en particular (en particular, la base de dades la formen mostres de *vinho verde*, un dels vins més importants de tot Portugal), acompanyat de 11 variables fisicoquímiques més. Aquestes variables són el resultat de tests objectius, menys la variable resposta, la qualitat, que és la mitjana de la valoració de 3 enòlegs experts. Cada expert va otorgar una puntuació entre el 0 (dolent) al 10 (excel.lent).

Les 12 variables del *dataset* són les següents:

- **fixed.acidity**: Acidesa fixada.
- **volatile.acidity**: Acidesa volàtil.
- **citric.acid**: Àcid cítric.
- **residual.sugar**: Sucre residual.
- **chlorides**: Clorurs.
- **free.sulfur.dioxide**: Diòxid de sofre lliure.
- **total.sulfur.dioxide**: Totat de diòxid de sofre.
- **density**: Densitat.
- **pH**: pH.
- **sulphates**: Sulfats.
- **alcohol**: Alcohol.
- **quality**: Qualitat. *Variable resposta*.

Pel que fa als *missing values*, no n'hi ha cap en aquesta base de dades, tal i com s'indica a la seva descripció oficial.

Abans de res, s'observen les característiques de cada una de les variables i els resultats són els següents.

```{r echo = FALSE}
Xid <- read.csv2("./data/winequality-red.csv")
X <- data.frame(Xid[,-1])
summary(X[,-12])
```

En general, tots els valors semblen normals. L'únic que crida l'atenció és que les variables $\texttt{residual.sugar}$, $\texttt{chlorides}$, $\texttt{sulphates}$ i $\texttt{total.sulfur.dioxide}$ prenen valors molt concentrats a l'extrem esquerre, ja que el tercer quartil i la mediana estan molt allunyats del màxim. Això pot voler dir que hi ha algun outlier que convé eliminar perquè no afecti al nostre anàlisi. Per detectar-ho, es realitza un $\texttt{boxplot}$ d'aquestes variables.

```{r echo = FALSE, fig.align = "center", out.height='70%', out.width='70%'}
par(mfrow=c(2,4))

boxplot(X[,5], main = "chlorides")
a <- which(X$chlorides > 0.15 | X$chlorides < 0.02)

boxplot(X[,4], main = "residual.sugar")
b <- which(X$residual.sugar > 5 | X$residual.sugar < 1)

boxplot(X[,7], main = "total.sulfur.dioxide")
c <- which(X$total.sulfur.dioxide > 150)

boxplot(X[,10], main = "sulphates")
d <- which(X$sulphates > 1.2)

outliers <- union(union(union(a,b),c),d)

cat("Total outliers = ")
length(outliers)

cat("% Outliers = ")
length(outliers)/1599 * 100
```

S'observa que, com era d'esperar, hi ha nombrosos *outliers* en aquestes variables. Donat que el total d'outliers suposa el 10% de les dades, es procedeix a eliminar-los per evitar possibles influències negatives en l'anàlisi. S'observa ara que els valors de les variables estan més repartits.

```{r echo = FALSE, fig.align = "center", out.height='70%', out.width='70%'}
X <- X[-outliers,]

par(mfrow=c(2,4))

boxplot(X[,5], main = "chlorides")
boxplot(X[,4], main = "residual.sugar")
boxplot(X[,7], main = "total.sulfur.dioxide")
boxplot(X[,10], main = "sulphates")
```

# Anàlisi de Components Principals (PCA)

## Breu síntesi

El primer anàlisi que de les dades que es farà és l'**Anàlisi de Components  (PCA)**. Els objectius fonamentals d'aquest estudi són reduir el nombre de variables i realitzar una representació en dues dimensions de les dades (__*biplot*__). El que es busca són combinacions lineals de les variables de la forma $F_i = a_{i1}X_1 + \cdots + a_{1N}X_pN$ tal que $F_1, \cdots, F_N$ (components principals) no estiguin correlats. Aquestes combinacions lineals ens les dóna la descomposició espectral de la matriu de les dades $F = X A$, on A és la matriu de vectors principals. Una vegada calculats aquests components, ens quedarem amb els suficients per explicar un 80% de la variància de les dades o, en el nostre cas, amb els que tinguin un **valor propi major que 1**.

Una vegada feta aquesta descomposició, es pot fer el __*biplot*__ de les dades a partir de $X = FA$. En aquesta representació, val a dir que les distàncies euclidianes entre els punts aproximen les distàncies Mahalanobis entre les observacions reals. A més, la llargada de les fletxes que representen cada variable és una estimació de la seva desviació estàndard. Ara bé, un dels aspectes més interesants és que l'**angle** entre les diferents fletxes representa una estimació de la correlació entre les dues variables, és a dir:

- Si dues fletxes són gairebé paral·leles i amb el mateix sentit, les variables estaran correlades positivament.
- Si dues fletxes són gairebé paral·leles i amb sentits oposats, les variables estaran correlades negativament.
- Si dues fletxes són gairebé ortogonals, les variables no estaran correlades.

Un altre aspecte a tenir en compte, és si s'utilitza la matriu de covariàncies o la de correlacions pels càlculs. Donat que aquestes dades tenen variables amb unitats de mesura diferents, s'utilitzarà la **matriu de correlacions** (motiu pel qual ens quedarem amb els components principals amb valors propis majors que 1), que és invariant respencte les unitats de mesura.

En resum, els objectius d'aquest anàlisi seràn la representació __*biplot*__ de les dades, la **correlació** entre les variables de la base de dades i els diferents **coeficients** dels components principals més representatius, ja que serà un indicador de les variables més importants.

## PCA de les nostres dades.

Abans de res, es construeix un nou *dataset* eliminant la variable resposta, és a dir, la variable $\texttt{quality}$. Tot seguit, es calcula la matriu de correlacions de les dades, així com el seu *scatter plot* per observar aquestes correlacions.

```{r echo = FALSE, fig.align = "center", out.height='50%', out.width='50%'}
db <- X[,-12]
cor(db)
length(which(abs(cor(db)) > 0.3)) - 11
```
```{r echo = FALSE, fig.align = "center"}
library(car)
library(FactoMineR)
library(ade4)
scatterplotMatrix(db)
```

S'observa, tant en la matriu de correlacions com en el plot de les variables que, en general, **no hi ha correlació entre elles**. De fet, la majoria de valors de la matriu de correlació són inferiors a 0.3. Només hi ha 4 casos que cal destacar:

- $\texttt{fixed.acidity}$ i $\texttt{density}$ estan positivament correlades: 0.697.
- $\texttt{fixed.acidity}$ i $\texttt{pH}$ estan negativament correlades: 0.714.
- $\texttt{fixed.acidity}$ i $\texttt{citric.acid}$ estan positivament correlades: 0.697.
- $\texttt{free.sulfur.dioxide}$ i $\texttt{free.sulfur.dioxide}$ estan positivament correlades: 0.646.

Ara bé, en cap d'aquests casos la correlació entre les variables en qüestió és major que 0.75. És a dir, que no són correlacions molt clares. Calculem ara els components principals que s'han explicat a l'apartat anterior.

```{r echo = FALSE, fig.align = "center", out.height='50%', out.width='50%'}
pca.out <- princomp(db, cor = T)
summary(pca.out)
```

S'observa que, per a explicar almenys un 80% de la variabilitat de les dades, es necessiten almenys **5 components principals**, tot i que el cinquè ja té un valor propi menor que 1. Això és degut a què, com s'ha vist a la matriu de correlacions, les variables són força independents, amb pocs casos de correlacions importants, cosa que implica que no podem reduir molt el nombre de variables. Una altra aspecte a destacar és que **no hi ha cap valor propi nul**, fet que ens indica que no hi ha cap variable que sigui combinació lineal directe de les altres.

A continuació es presenta la representació *biplot* corresponent a aquests components principals.

```{r echo = FALSE, fig.align = "center", out.height='50%', out.width='50%'}
biplot(pca.out, xlabs = rep(".", nrow(db), col = "black"))
screeplot(pca.out)
```

El primer que cal dir d'aquesta representació és que només explica el 47.47% de la variabilitat de les dades, ja que s'hi veuen representats només els dos primers components principals. Així doncs, aquesta representació no és del tot fiable. Alguns fets que ho demostres són que, per exemple, les fletxes de les varaibles $\texttt{residual.sugar}$ i $\texttt{density}$ són pràcticament paral.leles i, en canvi, la correlació entre aquestes dues varaibles és de 0.377. Això podria ser degut a què els 2 primers components principals no representen gaire la variabilitat d'aquestes variables i es centren més en altres.

Tot i així, es veur representada molt representada la correlació negativa de la variable $\texttt{fixed.acidity}$ i $\texttt{pH}$, fet que pot voler dir que tenen gran part de la seva variància explicada pels dos primers components. El mateix passsa amb la correlació positiva de $\texttt{fixed.acidity}$ i $\texttt{citric.acid}$.

A continuació, s'analitzen els coeficients dels components principals, per veure quines variables expliquen més.

```{r echo = FALSE}
pca.out$loadings
```

S'observa, com havíem intuït amb el *biplot*, que el primer component representa sobretot la variable $\texttt{fixed.acidity}$. També les variables $\texttt{citric.acid}$, $\texttt{density}$ i $\texttt{pH}$. Això explica que les correlacions que havíem destacat abans es vegin prou representades en el biplot. Una fet que crida l'atenció és que la component que més representa la variable  $\texttt{residual.sugar}$ és la tercera. Això explica que la seva representació en el biplot no fos fiable. Un altre fet destacable és que les variables $\texttt{free.sulfur.dioxide}$ i $\texttt{total.sulfur.dioxide}$ no tenen representació en el primer component.

# Inferència Multivariant

# Anàlisi Discriminant (LDA/QDA)

## Breu síntesi

En segon lloc, es farà un **Anàlisi Discriminant (LDA/QDA)**. Els objectius principals d'aquest estudi són  L'anàlisi de discriminant lineal (LDA en anglès) consisteix en trobar una funció discriminant tal que decideixi a quina classe $C_k$ pertany una observació $x$.

En LDA, assumim que les dades de les diferents $k$ classes provenen d'una distribució gaussiana i la seva funció de densitat es pot escriure com:
$$
f_k(x) = \frac{1}{\sqrt{2\pi\sigma}} \exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)
$$

on, per cada classe $k$, $\mu_k$ és la mitjana de la classe $k$ i $\sigma$ és la variància comuna de totes les dades (un altre requisit que assumim quan fem LDA).

El mètode assigna cada observació $x$ a la classe la qual $P_k(x)$ sigui major. Per aquesta tasca de maximització podem considerar la tasca equivalent de trobar el màxim prenent el logaritme i aleshores ens queda la funció discriminant que volíem:

$$
\delta_k(x) = x\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
$$

Com veiem, és una funció lineal en $x$.

En el nostre cas, però, ens interesseran les dades de distribucions Guassianes multivariants. L'equació del discriminant canvia introduïnt-hi notació vectorial:

$$
\delta_k(\mathbf{x}) = \mathbf{x}^\top \Sigma^{-1} \vec{\mu_k} - \frac{1}{2}\; \vec{\mu_k}^\top\; \Sigma^{-1}\; \vec{\mu_k} + \log(\vec{\pi_k})
$$

```{r echo = FALSE}
quality <- X[,12]
X <- data.matrix(db)
df <- data.frame(X[,-12])

D <- ncol(df)
N <- nrow(df)

df <- scale(df, center = TRUE, scale = TRUE)
```

Veiem això en R. Farem el LDA amb la llibreria $\texttt{MASS}$. Introduirem una nova variable $\texttt{quality.section}$ que agruparà els vins en dolents (qualificació entre 0 i 3), mediocres (entre 4 i 6) i bons (entre 7 i 10).

```{r echo=FALSE}
## Feature extraction
quality.low <- which(quality <= 4)
quality.med <- which(4 < quality & quality < 7)
quality.high <- which(7 <= quality)

quality.section <- rep(NA, N)
quality.section[quality.low] <- 0
quality.section[quality.med] <- 1
quality.section[quality.high] <- 2

hist(quality.section,
     breaks = seq(-0.5, 2.5, by = 1))
```

```{r}
library(MASS)
lda.out <- lda(quality.section ~ X)
lda.out$means
```

Com veiem, les mitjanes de totes les variables excepte $\texttt{density}$ són prou diferenciades. Podem veure com de bé ha discriminat el LDA amb la matriu de confusió:
```{r}
lda.pred <- predict(lda.out)
(lda.confusiontable <- table(quality.section, lda.pred$class))
sum(diag(lda.confusiontable))/sum(lda.confusiontable)
```

Veiem que distingeix perfectament els vins bons dels dolents però entre mediocres i els extrems hi ha més discrepància.
Això es deu a la gran predominància de vins mediocres dins les dades, que els ha donat un paper principal i l'anàlisi s'ha centrat a obtenir millors resultats agrupant vins mediocres.
Podríem baixar aquest biaix igualant les dades dels tres conjunts mitjançant la reducció de la classe mediocre i fer múltiples models amb les diferents dades.
```{r echo=FALSE}
summary(as.factor(quality.section))
N.dolents <- length(which(quality.section == 0))
N.mediocres <- length(which(quality.section == 1))
N.bons <- length(which(quality.section == 2))

random.mediocres <- quality.med[floor(runif(N.dolents, 1, N.mediocres + 1))]
random.bons <- quality.high[floor(runif(N.dolents, 1, N.bons + 1))]

quality.lite.index <- c(quality.low,
                        random.mediocres,
                        random.bons)

quality.lite <- quality[quality.lite.index]
N.lite <- length(quality.lite.index)

## Feature extraction
quality.lite.low <- which(quality.lite <= 4)
quality.lite.med <- which(4 < quality.lite & quality.lite < 7)
quality.lite.high <- which(7 <= quality.lite)

quality.lite.section <- rep(NA, N.lite)
quality.lite.section[quality.lite.low] <- 0
quality.lite.section[quality.lite.med] <- 1
quality.lite.section[quality.lite.high] <- 2

X.lite <- X[quality.lite.index, ]
hist(quality.lite.section,
     breaks = seq(-0.5, 2.5, by = 1))
```

Ara, amb aquesta nova distribució més equitativa de les dades, tornem a fer un LDA.

```{r}
df.lite <- data.frame(X.lite)
lda.lite.out <- lda(quality.lite.section ~ ., df.lite)
lda.lite.out$means
```

Veiem com ha funcionat ara:
```{r}
lda.lite.pred <- predict(lda.lite.out)
(lda.lite.confusiontable <- table(quality.lite.section, lda.lite.pred$class))
sum(diag(lda.lite.confusiontable))/sum(lda.lite.confusiontable)
```


# Anàlisi clúster

## Breu síntesi

Un altre dels anàlisis que realitzarem sobre les dades és l'**Anàlisi Clúster**, l'anàlisi d'agrupaments. Tal i com indica el seu nom, amb aquest estudi es buscaran els grups naturals que hi ha dins de les dades. Aquests grups estan formats per observacions "semblants" entre elles, on aquest nivell de semblança s'ha de definir prèviament i s'ha d'entendre com la distància entre les observacions. A més, aquest estudi ens permet reduir el conjunt de dades de $n$ observacions a $k$ grups prou semblans.

Pel que fa als algoritmes que s'utilitzen per trobar aquests grups, bàsicament els dividirem en 3 tipus: **jeràrquics**, **no jeràrquics** i els mètodes __*model-based*__. En relació als primers, val a dir que, un cop una observació s'ha assignat a un grup, ja no canvia de grup. L'algoritme més utilitzat és l'**aglomeració jeràrquica**, que simplement va ajuntant a cada pas les observacions/grups que estiguin a menor distància. Pel càlcul de la distància entre dos grups ja creats, es pot considerar:

- **Single linkage**: La distància entre les observacions més properes entre els dos grups. Sensible als outliers.
- **Complete linkage**: La distància entre les observacions més llunyanes entre els dos grups. Sensible als outliers.
- **Average linkage**: La mitjana de la distància entre les observacions dels dos grups. Menys sensible als outliers.
- **Centroid distance**: Es defineix la distància entre grups com $d_{rs}^2 = \sum_{j=1}^p (\bar{x}_{rj} - \bar{x}_{sj})^2$, on $r$ i $s$ són els dos grups. Menys sensible als outliers.
- **Ward's criterion**: Es defineix $\Delta = \frac{n_r n_s}{n_r + n_s}d_{rs}^2$ i s'agrupen els dos grups amb $\Delta$ mínim. Menys sensible als outliers.

Per altra banda, en els algoritmes **no jeràrquics**, una observació pot anar canviant de grup a mesura que avança el procediment. L'algoritme més utilitzat en aquest cas és el __*K-Means*__. La idea és anar calculant pas a pas els centres de cada grup i anar ajustant-los a les dades. Es comença amb un conjunt de centres aleatòri i s'assignen les observacions més properes a cada centre. A cada pas es recalculen els centres i es reassignen les dades. Quan en un pas ja no hi ha més reassignacions, s'acaba. En nombre de grups $K$ és fixat.

Per últim, així com tant els algoritmes jeràrquics o no jeràrquics no feien cap tipus d'assumpció sobre la distribució de les dades, en els mètodes **model-based** s'utilitzen models probabilísics concrets per agrupar les dades. La idea és que s'entenen les dades com un *finite mixture model* 

$$ g(x|\pi, \theta) = \pi_1f_1(x|\theta_1) + \cdots + \pi_kf_k(x|\theta_k)$$

on les $f_i$ són les distribucions de probabilitat de cada un dels $k$ grups, que normalment es té $f_i$ ~ $N(\mu, \Sigma)$. A partir d'aquesta assumpció, es calculen les probabilitats de que cada observació $x_i$ pertanyi al grup $i$ com

$$ \frac{\pi_i f_i(x_j|\theta_i)}{\sum_{i=1}^k \pi_i f_i(x_j|\theta_i)} $$

El procediment en aquests algoritmes probabilístics és estimar el *finite mixture model* amb màxima versemblança, calcular les probabilitats *a-posteriori* de cada observació a cada grup i assignar-la al grup on tingui una probabilitat més elevada.

## Anàlisi Clúster de les nostres dades

Tal i com ja s'havia fet abans, s'utilitzarà un *dataset* on s'ha extret la variable resposta $\texttt{quality}$. El primer que es fa és l'estandarització de les dades per aconseguir que totes tinguin mitjana zero, anomenada $X_s$. A continuació, es calcula la matriu de distàncies $D$ euclidianes entre les observacions de les dades i s'observa que la distància més gran és 12.98895.

```{r echo=FALSE}
Xs <- scale(db)
D <- dist(Xs, method = "euclidean")
```


A continuació, es procedeix a realitzar-se l'anàlisi clúster de les dades amb cada un dels mètodes explicats a l'apartat anterior.

### Anàlisi Cluster Jeràrquic

En primer lloc, es realitza l'aglomeració jeràrquica de les dades amb 4 tipus diferents de distàncies entre grups (indicades com a títol de cada *plot*). Donat que la variable resposta d'aquest anàlisi és la qualitat del vi, s'haurien de veure agrupacions d'observacions amb qualitats similars, ja que s'entén que si dues observacions tenen la mateixa qualitat, són observacions properes. Així doncs, donat que la qualitat s'ha valorat de l'1 al 10, s'haurien de detectar un nombre menor que 10 de grups clars

```{r echo = FALSE}
library(colorspace)
library(viridis)
library(dendextend)
n_quality <- length(unique(X$quality))
#cols <- c("red","green","yellow","blue", "black", "orange")
cols <- magma(n_quality)
col_quality <- cols[X$quality]
```

```{r echo = FALSE}
closest.neigh <- as.dendrogram(hclust(D, method = "single"))
col_quality <- col_quality[order.dendrogram(closest.neigh)]
closest.neigh <- set(closest.neigh, "labels_colors", col_quality)
plot(closest.neigh, main="Single linkage", cex = 0.3)

farthest.neigh <- as.dendrogram(hclust(D, method = "complete"))
col_quality <- col_quality[order.dendrogram(farthest.neigh)]
farthest.neigh <- set(farthest.neigh, "labels_colors", col_quality)
plot(farthest.neigh, main="Complete linkage", cex = 0.3)

average.neigh <- as.dendrogram(hclust(D, method = "average"))
col_quality <- col_quality[order.dendrogram(average.neigh)]
average.neigh <- set(average.neigh, "labels_colors", col_quality)
plot(average.neigh, main="Average linkage", cex = 0.3)

ward.neigh <- as.dendrogram(hclust(D, method = "ward.D2"))
col_quality <- col_quality[order.dendrogram(ward.neigh)]
ward.neigh <- set(ward.neigh, "labels_colors", col_quality)
plot(ward.neigh, main="Ward's criterion", cex = 0.3)
```

En el cas del *Single linkage*,  sembla ser que hi ha un grup molt gran i petits grupets. Això podria significar que la variabilitat de la qualitat de les observacions és petita, és a dir, que tenim moltes puntuacions iguals. Si es volguéssin agrupar les dades en un nombre més reduït de grups seguint aquesta agrlomeració, quedaria un grup amb un porcentatge molt elevat de les observacions i grups amb un percentatge molt petit. Així doncs, en aquest cas, no podem dir que resultin detectables les diferents qualitats de vi en aquesta agrupació. Per poder diferenciar un nombre raonable de grups, s'hauria de tallar a una distància de més de 3.

En el cas del *Complete linkage*, sembla ser que hi ha unes agrupacions de tamanys més iguals. De fet, es poden veure clarament tres grups, que es podria suposar que són agrupacions de vi de alta qualitat, vi de baixa qualitat i vi de qualitat mitjana. En aquest cas doncs, a diferència de l'altre, sembla que sí que són detectables les diferents qualitats del vi. Per poder diferenciar aquests 3 grups clars, hauríem de tallar a una distància de més de 10.

En el cas del *Average linkage*, es té un comportament semblant al de *Single linkage*: s'acaba formant un grup molt gran i grups més reduïts. Així que tampoc serveix per detectar les diferents qualitats del vi. Per poder diferenciar un nombre raonable de grups, s'hauria de tallar a una distància de més de 6.

Per últim, en el cas del *Ward's criterion* és el cas on més clar es veu que es formen grups mes o menys del mateix tamany i iguals. En concret, es veu que es formen 4 grups clars i, per tant, són detectables les diferents qualitats del vi. Per poder diferenciar un nombre raonable de grups, s'hauria de tallar a una distància de més de 40.

Ara bé, donat que s'han marcat de colors diferents les observacions, tenint en compte la seva puntuació, s'observa que a cada grup hi ha barreja de puntuacions. Això no és preocupant, ja que sabem que la puntuació ha estat posada de manera relativament subjectiva i pot no estar perfectament correlacionada amb el valor de les dades, que és en el que es basa aquest algoritme per fer els grups. Per poder diferenciar aquests 4 grups clars, s'hauria de tallar a una distància de més de 3. En definitiva, s'observa que dues observacions de qualitats prou diferents poden caure dins el mateix clúster. Un altre detall a remarcar, és que hi ha moltes observacions de la mateixa qualitat.

A continuació, s'analitza en més profunditat com han agrupat aquests algoritmes, tallant l'arbre en cada cas per formar un nombre en concret de clústers. En el primer cas, el *Single Linkage*, s'ha tallat l'arbre per obtenir 3 clústers i el resultat és bastant dolent, ja que com s'havia comentat abans, es crea un clúster molt grans i dos de molt petits (de 2 i 1 observació). A més, el clústers petits contenen observacions amb qualitat 5, que és la més abundant al clúster gran.

```{r echo = FALSE}
clusters.closest <- cutree(closest.neigh, 3)
table(clusters.closest)
(compar.closest <- table(clusters.closest, X$quality))
```

En el cas del *Complete Linkage*, s'observa que si es creen tres grups, les mides dels grups són bastant similars. Tot i que tots els grups contenen moltes observacions de qualitat 5 i 6, s'observa que les observacions de qualitat més baixa s'han concentrat al clúster 1 i les de qualitat més alta al clúster 2. Així doncs, s'ha realitzat una separació més o menys depenent de la qualitat. A més, en el clúster "d'alta qualitat" és on menys observacions de qualitat 5 hi ha i en el clúster de "baixa qualitat" és on més n'hi ha.

```{r echo = FALSE}
clusters.farthest <- cutree(farthest.neigh, 3)
table(clusters.farthest)
(compar.farthest <- table(clusters.farthest, X$quality))
```

En el cas de l'*Average Linkage*, es té una situació semblant a la del *Single Linkage*: un grup molt gran i dos de petits amb 3 i 1 observacions. Per tant, no es detecten bé les diferents qualitats del vi.

```{r echo = FALSE}
clusters.average <- cutree(average.neigh, 3)
table(clusters.average)
(compar.average <- table(clusters.average, X$quality))
```

Per últim, en el cas del *Ward Criterion*, s'observa que es poden crear 4 grups de tamanys similars. A més, al primer grup es concentren gariebé totes les observacions de pitjor qualitat (3 i 4), en el segon les de qualitat una mica millor (4 i 5), en el tercer és on abunden més les de qualitat mitjana-alta (6 i 7) i en l'últim és on hi ha la gran majora de qualitat alta (7 i 8). Per tant, aquest és el cas on més clarament s'agrupa segons la qualitat del vi.

```{r echo = FALSE}
clusters.ward <- cutree(ward.neigh, 4)
table(clusters.ward)
(compar.ward <- table(clusters.ward, X$quality))
```
Una aproximació de l'error en el cas del *Ward Criterion* és 44,84%.
```{r echo = FALSE}
ward.maxs <- apply(compar.ward, 1, which.max)
ward.misclass <- 0
for (k in 1:4) ward.misclass <- ward.misclass + sum(compar.ward[k,-ward.maxs[k]])
(ward.err <- ward.misclass/sum(compar.ward))
```

En conclusió, si s'estableixen com a criteris de distància entre grups el *Ward Criterion*, un algoritme d'agrupació jeràrquic és capaç d'agrupar les observacions més o menys per qualitat. També s'observa que les qualitats altes o les qualitats baixes són les que millor es detecten, cosa que fa pensar que ha d'haver-hi detalls prou significants en les dades fisicoquímiques per diferenciar un vi dolent d'un vi molt bo.

### Anàlisi Clúster No Jeràrquic

En segon lloc, s'aplicarà un algoritme no jeràrquic per agrupar les observacions. En concret, s'aplicarà l'algoritme **K-Means** amb $K=4$, per poder-ho comparar amb l'aglomeració jeràrquica amb el criteri de Ward utilitzada anteriorment.

```{r echo = FALSE}
set.seed(123)
clusters.kmeans <- kmeans(Xs, 4, nstart = 50)
clusters.kmeans$size
clusters.kmeans$centers
clusters.kmeans$betweenss / clusters.kmeans$totss

(compar.kmeans <- table(X$quality, clusters.kmeans$cluster))
```

S'observa que les dades es divideixen en 4 grups de tamanys similars. Pel que fa a la distribució d'observacions dintre de cada un dels grups tenim:

- Grup 1: Qualitat **baixa** (3-4): La majoria d'observacions de baixa qualitat es concentren el el primer grup. També veiem que s'hi concentren moltes observacions de qualitat 5, cosa que fa pensar que el pas de qualitat 4 a qualitat 5 és mes subjectiu que objectiu. Pel que fa a les variables que més es tenen en compte per calcular el centre d'aquest grup són la $\texttt{volatile.acidity}$, que es té en compte postivament (és a dir, com més, pitjor sembla ser el vi) i el $\texttt{citric.acid}$, que es té en compte positivament (és a dir, com menys, pitjor és el vi). 

- Grup 2: Qualitat **baixa-mitja** (4-5): La majoria d'observacions de qualitat 4 ja hem dit que estan al primer grup, però en el segon grup es concentren la majoria de les observacions restants de qualitats 4 i 5. S'observa que el que més es té en compte en aquest segon grup és les dues variables que es refereixen al Diòxid de Sofre. Les variables que abans marcaven una baixa qualitat del vi, no es tenen tant en compte en aquest grup.

- Grup 3: Qualitat **mitja-alta** (6-7): Una gran part de les observacions de qualitat 6-7 es concentra en aquest grup. Pel que fa a les variables que es destaquen, s'observa que es valora negativament la $\texttt{volatile.acidity}$, fet lògic ja que en els vins de baixa qualitat es valorava positivament, i es valora molt positivament el $\texttt{citric.acid}$, contrari també als vins de baixa qualitat. Un altre variable que sembla ser important en aquest grup és el $\texttt{fixed.acidity}$, que es valora positivament.

- Grup 4: Qualitat **alta** (7-8): Per últim, la majoria d'observacions de qualitat alta es concentren en aquest grup, on es valoren sobretot l'alcohol (variable $\texttt{alcohol}$), positivament, i la densitat (variable $\texttt{density}$) negativament. Ara bé, crida l'atenció que en aquest grup es valori negativament la densitat i en el grup anterior, de qualitat mitjana-alta, aquesta mateixa variable es valori positivament. El mateix passa amb la variable $\texttt{fixed.acidity}$. Ara bé, la variable $\texttt{volatile.acidity}$, igual que abans, es valora negativament. 

Totes aquestes inconsistències en la valoració de cada una de les variables a cada un dels grups es pot assignar a la subjectivitat de la valoració.

S'observa que, en aquest cas, la variabilitat entre els clusters comparada amb la variabilitat total de les dades és molt més reduïda. De fet, una aproximació de l'error és $ BetweenSS / TSS $, és a dir, la suma de quadrats entre els clústers entre la suma de quadrats total. En aquest cas, és 36,9%, inferior a l'aconseguida en el *Ward's Criterion*. Per tant, amb aquestes dades, *K-Means* funciona millor.

```{r}
kmeans.maxs <- apply(compar.kmeans, 1, which.max)
kmeans.misclass <- 0
for (k in 1:4) kmeans.misclass <- kmeans.misclass + sum(compar.kmeans[k,-kmeans.maxs[k]])
(kmeans.err <- kmeans.misclass/sum(compar.kmeans))
```

### Anàlisi Clúster *Model-Based*

Per últim, anem a agrupar les observacions seguint un model amb 4 mixtures, per poder-ho comparar amb la resta. El primer que s'observa és que els tamanys dels grups són bastant similars. A continuació s'analitzen cada un dels grups:

- Grup 1: Qualitat **baixa-mitja** (4-5): La majoria d'observacions de qualitat 4 o 5 es concentren en aquest primer grup. Una de les variables que es té més en compte és $\texttt{citric.acid}$, negativament. Això ja d'havia observat en l'anàlisi anterior, en el grup dels vins de qualitat baixa. Igual que abans, també es valora positivament el $\texttt{volatile.acidity}$. El $\texttt{fixed acidity}$ es valora negativament (abans s'havia vist que en els vins bons es valorava positivament). 

- Grup 2: Qualitat **alta** (7-8): La majoria de vins d'aquest grup són d'alta qualitat (7-8). Tal i com haviem vist abans, es valoren positivament el $\texttt{fixed.acidity}$ i el $\texttt{citric.acid}$, cosa que encaixa amb el que s'ha vist al grup anterior, de vins de qualitat més baixa. Observem que també, a diferència del grup anterior, el $\texttt{volatile.acidity}$ es valora negativament. En l'anàlisi del *K-Means* s'havia vist que la variable $\texttt{density}$ es valorava positivament en els bons vins i, en aquest cas, veiem que es compleix.

- Grup 3: Qualitat **mitja-alta** (5-6-7): Els vins de qualitat mitja es concentren en aquest grup, on hi ha poc vins de qualitat extremes (baixes o altes). Igual que abans, observem que tenen un pes important positivament les variables relacionades amb el Diòxid de Sofre. Donat que la qualitat és mitjana, les variables que estem detectant que marquen la qualitat del vi, no es tenen tant en compte en aquest grup.

- Grup 4: Qualitat **baixa** (3-4): Els pitjors vins es concentren en aquest grup, tot i que és el grup de tamany menor. Tornem a observar que es valora positivament la variable $\texttt{volatile.acidity}$, igual que en l'altre grup de vins de baixa qualitat. Un detall que cria l'atenció és la valoriació positiva de l'alcohol en aquest grup. Aquest fet s'explica perquè s'observa que també s'han incluït vins de qualitat molt bona en aquest grup, cosa que ha fet que pugi la mitjana.


Tal i com s'ha comentat abans, el fet que apareguin vins de qualiat màxima i mínima en un mateix grup és degut a la subjectivitat de la valoració.


```{r echo = FALSE}
library(mclust)
cluster.mix <- Mclust(Xs, 4)
```

```{r echo = FALSE}
sum <- summary(cluster.mix, parameters = T)
sum$mean
plot(cluster.mix, what = "classification")
(compar.mix <- table(cluster.mix$classification, X$quality))
mix.maxs <- apply(compar.mix, 1, which.max)

mix.misclass <- 0
for (k in 1:4) mix.misclass <- mix.misclass + sum(compar.mix[k,-mix.maxs[k]])
(mix.err <- mix.misclass/sum(compar.mix))
```



# Conclusions

# Referències