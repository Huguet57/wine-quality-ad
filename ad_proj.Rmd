---
title: 'PROJECTE ANÀLISI DE DADES: Wine Quality'
author: "David Anglada Rotger i Andreu Huguet Segarra"
date: "17/5/2019"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    number_sections: yes
    toc: yes
fig_width: 5
fig_height: 3
---

# Introducció

Avui en dia, el rang de consumidors del vi s'ha amplitat moltíssims, convertint-se amb una beguda que es pot trobar a totes les cases i, fins i tot, comparable amb la cervesa. Això ha provocat que l'interés per aquest producte hagi augmentat els darrers anys, provocant també un creixement de la indústria de la vineria. Com a conseqüència, el nombre d'investigacions i estudis que tenen com a finalitat la millora de la qualitat del vi o la pujada de les seves vendes s'ha disparat.

Un dels temes que preocupa més a aquest sector és la **certificació de qualitat**, un aspecte que depèn profundament de la catació i valoració d'enòlegs experts. Així doncs, la finalitat d'aquest estudi és estudiar quines variables afecten més a la qualitat final del vi i de quina manera influeixen.

Donat que algunes d'aquestes variables es poden controlar durant el procés de producció, serà interessant veure les que influeixen possitivament amb l'objectiu de potenciar-les, o detectar les que influeixen negativament per poder intentar neutralitzar el seu efecte.

# Descripció de la Base de Dades

Per la realització d'aquest estudi es disposa d'una base de dades amb 1599 entrades diferents. Cada una es correspon amb la certificació de qualitat d'un vi en particular (en particular, la base de dades la formen mostres de *vinho verde*, un dels vins més importants de tot Portugal), acompanyat de 11 variables fisicoquímiques més. Aquestes variables són el resultat de tests objectius, menys la variable resposta, la qualitat, que és la mitjana de la valoració de 3 enòlegs experts. Cada expert va otorgar una puntuació entre el 0 (dolent) al 10 (excel.lent).

Les 12 variables del *dataset* són les següents:

- **fixed.acidity**: Acidesa fixada.
- **volatile.acidity**: Acidesa volàtil.
- **citric.acid**: Àcid cítric.
- **residual.sugar**: Sucre residual.
- **chlorides**: Clorurs.
- **free.sulfur.dioxide**: Diòxid de sofre lliure.
- **total.sulfur.dioxide**: Totat de diòxid de sofre.
- **density**: Densitat.
- **pH**: pH.
- **sulphates**: Sulfats.
- **alcohol**: Alcohol.
- **quality**: Qualitat. *Variable resposta*.

Pel que fa als *missing values*, no n'hi ha cap en aquesta base de dades, tal i com s'indica a la seva descripció oficial.

Abans de res, s'observen les característiques de cada una de les variables i els resultats són els següents.

```{r echo = FALSE}
X <- read.csv2("winequality-red.csv")
summary(X[,-12])
```

En general, tots els valors semblen normals. L'únic que crida l'atenció és que les variables $\texttt{residual.sugar}$, $\texttt{chlorides}$, $\texttt{sulphates}$ i $\texttt{total.sulfur.dioxide}$ prenen valors molt concentrats a l'extrem esquerre, ja que el tercer quartil i la mediana estan molt allunyats del màxim. Això pot voler dir que hi ha algun outlier que convé eliminar perquè no afecti al nostre anàlisi. Per detectar-ho, es realitza un $\texttt{boxplot}$ d'aquestes variables.

```{r echo = FALSE, fig.align = "center", out.height='70%', out.width='70%'}
par(mfrow=c(2,4))

boxplot(X[,5], main = "chlorides")
a <- which(X$chlorides > 0.15 | X$chlorides < 0.02)

boxplot(X[,4], main = "residual.sugar")
b <- which(X$residual.sugar > 5 | X$residual.sugar < 1)

boxplot(X[,7], main = "total.sulfur.dioxide")
c <- which(X$total.sulfur.dioxide > 150)

boxplot(X[,10], main = "sulphates")
d <- which(X$sulphates > 1.2)

outliers <- union(union(union(a,b),c),d)

cat("Total outliers = ")
length(outliers)

cat("% Outliers = ")
length(outliers)/1599 * 100
```

S'observa que, com era d'esperar, hi ha nombrosos *outliers* en aquestes variables. Donat que el total d'outliers suposa el 10% de les dades, es procedeix a eliminar-los per evitar possibles influències negatives en l'anàlisi. S'observa ara que els valors de les variables estan més repartits.

```{r echo = FALSE, fig.align = "center", out.height='70%', out.width='70%'}
X <- X[-outliers,]

par(mfrow=c(2,4))

boxplot(X[,5], main = "chlorides")
boxplot(X[,4], main = "residual.sugar")
boxplot(X[,7], main = "total.sulfur.dioxide")
boxplot(X[,10], main = "sulphates")
```

# Anàlisi de Components Principals (PCA)

## Breu síntesi

El primer anàlisi que de les dades que es farà és l'**Anàlisi de Components  (PCA)**. Els objectius fonamentals d'aquest estudi són reduir el nombre de variables i realitzar una representació en dues dimensions de les dades (__*biplot*__). El que es busca són combinacions lineals de les variables de la forma $F_i = a_{i1}X_1 + \cdots + a_{1N}X_pN$ tal que $F_1, \cdots, F_N$ (components principals) no estiguin correlats. Aquestes combinacions lineals ens les dóna la descomposició espectral de la matriu de les dades $F = X A$, on A és la matriu de vectors principals. Una vegada calculats aquests components, ens quedarem amb els suficients per explicar un 80% de la variància de les dades o, en el nostre cas, amb els que tinguin un **valor propi major que 1**.

Una vegada feta aquesta descomposició, es pot fer el __*biplot*__ de les dades a partir de $X = FA$. En aquesta representació, val a dir que les distàncies euclidianes entre els punts aproximen les distàncies Mahalanobis entre les observacions reals. A més, la llargada de les fletxes que representen cada variable és una estimació de la seva desviació estàndard. Ara bé, un dels aspectes més interesants és que l'**angle** entre les diferents fletxes representa una estimació de la correlació entre les dues variables, és a dir:

- Si dues fletxes són gairebé paral·leles i amb el mateix sentit, les variables estaran correlades positivament.
- Si dues fletxes són gairebé paral·leles i amb sentits oposats, les variables estaran correlades negativament.
- Si dues fletxes són gairebé ortogonals, les variables no estaran correlades.

Un altre aspecte a tenir en compte, és si s'utilitza la matriu de covariàncies o la de correlacions pels càlculs. Donat que aquestes dades tenen variables amb unitats de mesura diferents, s'utilitzarà la **matriu de correlacions** (motiu pel qual ens quedarem amb els components principals amb valors propis majors que 1), que és invariant respencte les unitats de mesura.

En resum, els objectius d'aquest anàlisi seràn la representació __*biplot*__ de les dades, la **correlació** entre les variables de la base de dades i els diferents **coeficients** dels components principals més representatius, ja que serà un indicador de les variables més importants.

## PCA de les nostres dades.

Abans de res, es construeix un nou *dataset* eliminant la variable resposta, és a dir, la variable $\texttt{quality}$. Tot seguit, es calcula la matriu de correlacions de les dades, així com el seu *scatter plot* per observar aquestes correlacions.

```{r echo = FALSE, fig.align = "center", out.height='50%', out.width='50%'}
db <- X[,-12]
cor(db)
length(which(abs(cor(db)) > 0.3)) - 11
```
```{r echo = FALSE, fig.align = "center"}
library(car)
library(FactoMineR)
library(ade4)
scatterplotMatrix(db)
```

S'observa, tant en la matriu de correlacions com en el plot de les variables que, en general, **no hi ha correlació entre elles**. De fet, la majoria de valors de la matriu de correlació són inferiors a 0.3. Només hi ha 4 casos que cal destacar:

- $\texttt{fixed.acidity}$ i $\texttt{density}$ estan positivament correlades: 0.697.
- $\texttt{fixed.acidity}$ i $\texttt{pH}$ estan negativament correlades: 0.714.
- $\texttt{fixed.acidity}$ i $\texttt{citric.acid}$ estan positivament correlades: 0.697.
- $\texttt{free.sulfur.dioxide}$ i $\texttt{free.sulfur.dioxide}$ estan positivament correlades: 0.646.

Ara bé, en cap d'aquests casos la correlació entre les variables en qüestió és major que 0.75. És a dir, que no són correlacions molt clares. Calculem ara els components principals que s'han explicat a l'apartat anterior.

```{r echo = FALSE, fig.align = "center", out.height='50%', out.width='50%'}
pca.out <- princomp(db, cor = T)
summary(pca.out)
```

S'observa que, per a explicar almenys un 80% de la variabilitat de les dades, es necessiten almenys **5 components principals**, tot i que el cinquè ja té un valor propi menor que 1. Això és degut a què, com s'ha vist a la matriu de correlacions, les variables són força independents, amb pocs casos de correlacions importants, cosa que implica que no podem reduir molt el nombre de variables. Una altra aspecte a destacar és que **no hi ha cap valor propi nul**, fet que ens indica que no hi ha cap variable que sigui combinació lineal directe de les altres.

A continuació es presenta la representació *biplot* corresponent a aquests components principals.

```{r echo = FALSE, fig.align = "center", out.height='50%', out.width='50%'}
biplot(pca.out, xlabs = rep(".", nrow(db), col = "black"))
screeplot(pca.out)
```

El primer que cal dir d'aquesta representació és que només explica el 47.47% de la variabilitat de les dades, ja que s'hi veuen representats només els dos primers components principals. Així doncs, aquesta representació no és del tot fiable. Alguns fets que ho demostres són que, per exemple, les fletxes de les varaibles $\texttt{residual.sugar}$ i $\texttt{density}$ són pràcticament paral.leles i, en canvi, la correlació entre aquestes dues varaibles és de 0.377. Això podria ser degut a què els 2 primers components principals no representen gaire la variabilitat d'aquestes variables i es centren més en altres.

Tot i així, es veur representada molt representada la correlació negativa de la variable $\texttt{fixed.acidity}$ i $\texttt{pH}$, fet que pot voler dir que tenen gran part de la seva variància explicada pels dos primers components. El mateix passsa amb la correlació positiva de $\texttt{fixed.acidity}$ i $\texttt{citric.acid}$.

A continuació, s'analitzen els coeficients dels components principals, per veure quines variables expliquen més.

```{r echo = FALSE}
pca.out$loadings
```

S'observa, com havíem intuït amb el *biplot*, que el primer component representa sobretot la variable $\texttt{fixed.acidity}$. També les variables $\texttt{citric.acid}$, $\texttt{density}$ i $\texttt{pH}$. Això explica que les correlacions que havíem destacat abans es vegin prou representades en el biplot. Una fet que crida l'atenció és que la component que més representa la variable  $\texttt{residual.sugar}$ és la tercera. Això explica que la seva representació en el biplot no fos fiable. Un altre fet destacable és que les variables $\texttt{free.sulfur.dioxide}$ i $\texttt{total.sulfur.dioxide}$ no tenen representació en el primer component.

# Inferència Multivariant

# Anàlisi Discriminant (LDA/QDA)

## Breu síntesi

En segon lloc, es farà un **Anàlisi Discriminant (LDA/QDA)**. Els objectius principals d'aquest estudi són  L'anàlisi de discriminant lineal (LDA en anglès) consisteix en trobar una funció discriminant tal que decideixi a quina classe $C_k$ pertany una observació $x$.

En LDA, assumim que les dades de les diferents $k$ classes provenen d'una distribució gaussiana i la seva funció de densitat es pot escriure com:
$$
f_k(x) = \frac{1}{\sqrt{2\pi\sigma}} \exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)
$$

on, per cada classe $k$, $\mu_k$ és la mitjana de la classe $k$ i $\sigma$ és la variància comuna de totes les dades (un altre requisit que assumim quan fem LDA).

El mètode assigna cada observació $x$ a la classe la qual $P_k(x)$ sigui major. Per aquesta tasca de maximització podem considerar la tasca equivalent de trobar el màxim prenent el logaritme i aleshores ens queda la funció discriminant que volíem:

$$
\delta_k(x) = x\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
$$

Com veiem, és una funció lineal en $x$.

En el nostre cas, però, ens interesseran les dades de distribucions Guassianes multivariants. L'equació del discriminant canvia introduïnt-hi notació vectorial:

$$
\delta_k(\mathbf{x}) = \mathbf{x}^\top \Sigma^{-1} \vec{\mu_k} - \frac{1}{2}\; \vec{\mu_k}^\top\; \Sigma^{-1}\; \vec{\mu_k} + \log(\vec{\pi_k})
$$

```{r echo = FALSE}
quality <- X[,12]
X <- data.matrix(db)
df <- data.frame(X[,-12])

D <- ncol(df)
N <- nrow(df)

df <- scale(df, center = TRUE, scale = TRUE)
```

Veiem això en R. Farem el LDA amb la llibreria $\texttt{MASS}$. Introduirem una nova variable $\texttt{quality.section}$ que agruparà els vins en dolents (qualificació entre 0 i 3), mediocres (entre 4 i 6) i bons (entre 7 i 10).

```{r echo=FALSE}
## Feature extraction
quality.low <- which(quality <= 4)
quality.med <- which(4 < quality & quality < 7)
quality.high <- which(7 <= quality)

quality.section <- rep(NA, N)
quality.section[quality.low] <- 0
quality.section[quality.med] <- 1
quality.section[quality.high] <- 2

hist(quality.section,
     breaks = seq(-0.5, 2.5, by = 1))
```

```{r}
library(MASS)
lda.out <- lda(quality.section ~ X)
lda.out$means
```

Com veiem, les mitjanes de totes les variables excepte $\texttt{density}$ són prou diferenciades. Podem veure com de bé ha discriminat el LDA amb la matriu de confusió:
```{r}
lda.pred <- predict(lda.out)
(lda.confusiontable <- table(quality.section, lda.pred$class))
sum(diag(lda.confusiontable))/sum(lda.confusiontable)
```

Veiem que distingeix perfectament els vins bons dels dolents però entre mediocres i els extrems hi ha més discrepància.
Això es deu a la gran predominància de vins mediocres dins les dades, que els ha donat un paper principal i l'anàlisi s'ha centrat a obtenir millors resultats agrupant vins mediocres.
Podríem baixar aquest biaix igualant les dades dels tres conjunts mitjançant la reducció de la classe mediocre i fer múltiples models amb les diferents dades.
```{r echo=FALSE}
summary(as.factor(quality.section))
N.dolents <- length(which(quality.section == 0))
N.mediocres <- length(which(quality.section == 1))
N.bons <- length(which(quality.section == 2))

random.mediocres <- quality.med[floor(runif(N.dolents, 1, N.mediocres + 1))]
random.bons <- quality.high[floor(runif(N.dolents, 1, N.bons + 1))]

quality.lite.index <- c(quality.low,
                        random.mediocres,
                        random.bons)

quality.lite <- quality[quality.lite.index]
N.lite <- length(quality.lite.index)

## Feature extraction
quality.lite.low <- which(quality.lite <= 4)
quality.lite.med <- which(4 < quality.lite & quality.lite < 7)
quality.lite.high <- which(7 <= quality.lite)

quality.lite.section <- rep(NA, N.lite)
quality.lite.section[quality.lite.low] <- 0
quality.lite.section[quality.lite.med] <- 1
quality.lite.section[quality.lite.high] <- 2

X.lite <- X[quality.lite.index, ]
hist(quality.lite.section,
     breaks = seq(-0.5, 2.5, by = 1))
```

Ara, amb aquesta nova distribució més equitativa de les dades, tornem a fer un LDA.

```{r}
df.lite <- data.frame(X.lite)
lda.lite.out <- lda(quality.lite.section ~ ., df.lite)
lda.lite.out$means
```

Veiem com ha funcionat ara:
```{r}
lda.lite.pred <- predict(lda.lite.out)
(lda.lite.confusiontable <- table(quality.lite.section, lda.lite.pred$class))
sum(diag(lda.lite.confusiontable))/sum(lda.lite.confusiontable)
```


# Anàlisi clúster



# Conclusions

# Referències